# üîß **COMPLETE FEATURE ENGINEERING SPECIFICATION (REVISED)**

---

## üìã **FEATURE ENGINEERING OVERVIEW**

### **Key Principles**

1. **Forecast Compatibility**: All features must work with BOTH:
   - Dense historical tracks (3-6hr intervals, complete lifecycle)
   - Sparse forecast tracks (12-24hr intervals, only future positions)

2. **Aggregation Strategy**: Temporal sequences ‚Üí Single feature vector per storm-province pair

3. **Missing Data Handling**: Features must gracefully handle:
   - Missing storm intensity data (wind/pressure)
   - Incomplete wind radii
   - Variable-length storm tracks

4. **Target Alignment**: Features describe the ENTIRE storm event (not point-in-time), matching cumulative impact reports

5. **No Data Leakage**: Historical features MUST use only pre-storm data

---

## **FEATURE GROUP SUMMARY**

| Group | Features | Completeness | Importance | Status |
|-------|----------|--------------|------------|--------|
| **Group 1: Distance & Proximity** | 26 | ‚úÖ 100% | üî¥ Critical | ‚úÖ Complete |
| **Group 2: Weather Exposure** | 22 | ‚úÖ 100% | üî¥ Critical | ‚úÖ Complete |
| **Group 3: Storm Intensity** | 10 | ‚ö†Ô∏è 40-70% | üü° Medium | ‚úÖ Complete |
| **Group 4: Province Vulnerability** | 12 | ‚úÖ 100% | üü† High | üÜï NEW |
| **Group 5: Temporal & Seasonal** | 10 | ‚úÖ 100% | üü° Medium | üÜï NEW |
| **Group 6: Storm Motion** | 13 | ‚úÖ 95% | üü† High | üÜï NEW |
| **Group 7: Interactions** | 11 | ‚úÖ 100% | üü† High | üÜï NEW |
| **TOTAL** | **104** | **~90%** | - | - |

---

# GROUPS 1-3: See Original Feature_engineering.md (Lines 1-679)

For Groups 1-3 (Distance, Weather, Storm Intensity), refer to the original specification.
This document contains the **ADDITIONAL GROUPS** that were missing.

---

## üèòÔ∏è **GROUP 4: PROVINCE VULNERABILITY FEATURES**

**Purpose**: Capture inherent susceptibility of each province to typhoon impacts

**Why These Matter**: Two provinces at the same distance from the same storm can have vastly different impacts due to population density, infrastructure, geography, and historical vulnerability patterns.

---

### **4.1 Static Province Characteristics**

| Feature Name | Description | Unit | Data Source | Update Frequency |
|--------------|-------------|------|-------------|------------------|
| `population` | Total population | Persons | PSA Census | Annual/5-year |
| `population_density` | People per km¬≤ | Persons/km¬≤ | PSA | Annual/5-year |
| `land_area_km2` | Province land area | km¬≤ | NAMRIA | Static |
| `is_coastal` | Has coastline exposure | Boolean (0/1) | Geographic analysis | Static |
| `elevation_mean_m` | Average elevation | Meters | DEM data | Static |
| `urbanization_rate` | % urban population | Percentage | PSA | 5-year |

**Implementation:**
```python
def load_static_province_features(province_name, year, province_database):
    """
    Load time-invariant or slowly-changing province characteristics
    
    Parameters:
    province_name: Name of province
    year: Year of storm event (for population data)
    province_database: DataFrame with province characteristics
    """
    province_data = province_database[
        (province_database['province'] == province_name) & 
        (province_database['year'] <= year)
    ].iloc[-1]  # Get most recent data before storm
    
    return {
        'population': province_data['population'],
        'population_density': province_data['population_density'],
        'land_area_km2': province_data['land_area_km2'],
        'is_coastal': province_data['is_coastal'],
        'elevation_mean_m': province_data.get('elevation_mean_m', 100),  # Default if missing
        'urbanization_rate': province_data.get('urbanization_rate', 0.5)
    }
```

---

### **4.2 Historical Vulnerability Features**

**‚ö†Ô∏è Critical**: These must be computed using ONLY data BEFORE the current storm to avoid data leakage!

| Feature Name | Description | Computation | Purpose |
|--------------|-------------|-------------|---------|
| `historical_avg_affected` | Mean affected persons per storm | Mean of past impacts | Baseline vulnerability |
| `historical_avg_destroyed` | Mean destroyed houses per storm | Mean of past impacts | Infrastructure fragility |
| `historical_max_affected` | Worst recorded impact | Max of past impacts | Disaster potential |
| `storms_past_5yr` | Storm exposure frequency | Count in last 5 years | Cumulative stress |
| `years_since_last_major` | Time since >50k affected | Years elapsed | Recovery status |
| `vulnerability_index` | Normalized vulnerability score | (avg_affected / population) √ó 1000 | Per-capita risk |

**Implementation:**
```python
def compute_historical_vulnerability_features(province_name, current_storm_date, impact_history, province_population):
    """
    Compute province vulnerability based on PAST impacts only
    
    Parameters:
    province_name: Name of province
    current_storm_date: Date of current storm (to filter history)
    impact_history: DataFrame with all historical impacts
    province_population: Current population for normalization
    """
    # Filter to only impacts BEFORE current storm
    past_impacts = impact_history[
        (impact_history['province'] == province_name) & 
        (impact_history['date'] < current_storm_date)
    ]
    
    if len(past_impacts) == 0:
        # No historical data - use conservative defaults
        return {
            'historical_avg_affected': 0,
            'historical_avg_destroyed': 0,
            'historical_max_affected': 0,
            'storms_past_5yr': 0,
            'years_since_last_major': 999,
            'vulnerability_index': 0
        }
    
    # Calculate features
    avg_affected = past_impacts['affected_persons'].mean()
    avg_destroyed = past_impacts['destroyed_houses'].mean()
    max_affected = past_impacts['affected_persons'].max()
    
    # Storms in last 5 years
    five_years_ago = current_storm_date - pd.Timedelta(days=5*365)
    recent_storms = past_impacts[past_impacts['date'] > five_years_ago]
    storms_5yr = len(recent_storms)
    
    # Years since major disaster
    major_disasters = past_impacts[past_impacts['affected_persons'] > 50000]
    if len(major_disasters) > 0:
        last_major = major_disasters['date'].max()
        years_since_major = (current_storm_date - last_major).days / 365.25
    else:
        years_since_major = 999
    
    # Vulnerability index (per-capita historical impact)
    vulnerability_index = (avg_affected / province_population * 1000) if province_population > 0 else 0
    
    return {
        'historical_avg_affected': avg_affected,
        'historical_avg_destroyed': avg_destroyed,
        'historical_max_affected': max_affected,
        'storms_past_5yr': storms_5yr,
        'years_since_last_major': years_since_major,
        'vulnerability_index': vulnerability_index
    }
```

---

## **GROUP 4 SUMMARY: 12 Province Vulnerability Features**

| Subgroup | Feature Count | Key Features |
|----------|---------------|--------------|
| Static Characteristics | 6 | population, population_density, is_coastal, elevation |
| Historical Vulnerability | 6 | historical_avg_affected, storms_past_5yr, vulnerability_index |

**Critical Note**: Historical features MUST use only pre-storm data to avoid temporal leakage!

---

## üìÖ **GROUP 5: TEMPORAL & SEASONAL FEATURES**

**Purpose**: Capture seasonal patterns and temporal context of typhoon occurrence

**Why These Matter**: Typhoon impacts vary significantly by season (peak season storms vs. early/late season), time of year (harvest season, school calendar), and climate patterns (ENSO phases).

---

### **5.1 Calendar Features**

| Feature Name | Description | Values | Forecast Compatible? |
|--------------|-------------|--------|---------------------|
| `month` | Month of year | 1-12 | ‚úÖ YES |
| `season` | Meteorological season | 1-4 (DJF, MAM, JJA, SON) | ‚úÖ YES |
| `is_peak_typhoon_season` | Aug-Nov peak season | Boolean (0/1) | ‚úÖ YES |
| `day_of_year` | Day within year | 1-366 | ‚úÖ YES |
| `quarter` | Calendar quarter | 1-4 | ‚úÖ YES |

**Implementation:**
```python
def compute_calendar_features(storm_date):
    """
    Extract calendar-based temporal features
    
    Parameters:
    storm_date: datetime object for storm date
    """
    month = storm_date.month
    
    # Meteorological seasons (Northern Hemisphere)
    if month in [12, 1, 2]:
        season = 1  # DJF (Winter)
    elif month in [3, 4, 5]:
        season = 2  # MAM (Spring)
    elif month in [6, 7, 8]:
        season = 3  # JJA (Summer)
    else:
        season = 4  # SON (Fall)
    
    # Peak typhoon season in Philippines: Aug-Nov
    is_peak_season = 1 if month in [8, 9, 10, 11] else 0
    
    return {
        'month': month,
        'season': season,
        'is_peak_typhoon_season': is_peak_season,
        'day_of_year': storm_date.timetuple().tm_yday,
        'quarter': (month - 1) // 3 + 1
    }
```

---

### **5.2 Storm Sequence Features**

| Feature Name | Description | Computation | Purpose |
|--------------|-------------|-------------|---------|
| `storm_number_this_year` | Nth storm of the year | Sequential count | Season progression |
| `days_since_last_storm` | Days since previous storm | Time difference | Recovery time |
| `cumulative_storms_this_season` | Total storms so far this year | Running count | Cumulative fatigue |

**Implementation:**
```python
def compute_storm_sequence_features(storm_date, storm_history):
    """
    Features based on storm's position in seasonal sequence
    
    Parameters:
    storm_date: Date of current storm
    storm_history: DataFrame with all historical storms
    """
    year = storm_date.year
    
    # Storms in current year before this storm
    storms_this_year = storm_history[
        (storm_history['year'] == year) & 
        (storm_history['date'] < storm_date)
    ]
    
    storm_number = len(storms_this_year) + 1
    cumulative_count = storm_number
    
    # Days since last storm
    if len(storms_this_year) > 0:
        last_storm_date = storms_this_year['date'].max()
        days_since_last = (storm_date - last_storm_date).days
    else:
        days_since_last = 365  # First storm of year
    
    return {
        'storm_number_this_year': storm_number,
        'days_since_last_storm': days_since_last,
        'cumulative_storms_this_season': cumulative_count
    }
```

---

### **5.3 Climate Pattern Features** (Optional Enhancement)

| Feature Name | Description | Data Source | Values |
|--------------|-------------|-------------|--------|
| `enso_phase` | El Ni√±o/La Ni√±a/Neutral | NOAA ONI | -1, 0, 1 |
| `oni_index` | Oceanic Ni√±o Index | NOAA | -3 to +3 |

**Implementation:**
```python
def get_enso_phase(storm_date, oni_data):
    """
    Get ENSO phase for storm date
    
    Parameters:
    storm_date: Date of storm
    oni_data: DataFrame with ONI index values
    """
    # Match to 3-month average period
    oni_value = oni_data[oni_data['date'] <= storm_date].iloc[-1]['oni']
    
    if oni_value >= 0.5:
        enso_phase = 1  # El Ni√±o
    elif oni_value <= -0.5:
        enso_phase = -1  # La Ni√±a
    else:
        enso_phase = 0  # Neutral
    
    return {
        'enso_phase': enso_phase,
        'oni_index': oni_value
    }
```

---

## **GROUP 5 SUMMARY: 10 Temporal Features**

| Subgroup | Feature Count | Key Features |
|----------|---------------|--------------|
| Calendar | 5 | month, season, is_peak_season |
| Storm Sequence | 3 | storm_number_this_year, days_since_last_storm |
| Climate Patterns | 2 | enso_phase, oni_index (optional) |

---

## üåÄ **GROUP 6: STORM MOTION & EVOLUTION FEATURES**

**Purpose**: Capture how the storm moves and changes, not just where it is

**Why These Matter**: A slow-moving storm dumps more rain. A recurving storm hits unexpected areas. An accelerating storm may weaken faster.

---

### **6.1 Forward Motion Features**

| Feature Name | Description | Computation | Units | Forecast Compatible? |
|--------------|-------------|-------------|-------|---------------------|
| `mean_forward_speed` | Average storm translation speed | Mean of STORM_SPEED | km/h | ‚úÖ YES |
| `max_forward_speed` | Peak movement speed | Max of STORM_SPEED | km/h | ‚úÖ YES |
| `min_forward_speed` | Slowest movement | Min of STORM_SPEED | km/h | ‚úÖ YES |
| `speed_at_closest_approach` | Movement speed when nearest | STORM_SPEED at min_distance | km/h | ‚úÖ YES |
| `is_slow_moving` | Nearly stationary storm | Boolean (speed < 15 km/h) | 0/1 | ‚úÖ YES |
| `is_fast_moving` | Rapidly translating | Boolean (speed > 40 km/h) | 0/1 | ‚úÖ YES |

**Implementation:**
```python
def compute_forward_motion_features(track_points, province_centroid):
    """
    Analyze storm translation speed
    
    Parameters:
    track_points: List of storm position records with STORM_SPEED
    province_centroid: Province location for closest approach calculation
    """
    # Extract valid speed data
    speeds = [p.STORM_SPEED for p in track_points 
              if hasattr(p, 'STORM_SPEED') and not np.isnan(p.STORM_SPEED)]
    
    if len(speeds) == 0:
        # Fallback: calculate from positions
        speeds = []
        for i in range(1, len(track_points)):
            dist = haversine(
                track_points[i-1].lat, track_points[i-1].lon,
                track_points[i].lat, track_points[i].lon
            )
            time_diff = (track_points[i].timestamp - track_points[i-1].timestamp).total_seconds() / 3600
            if time_diff > 0:
                speeds.append(dist / time_diff)
    
    if len(speeds) == 0:
        return {
            'mean_forward_speed': 20,  # Default
            'max_forward_speed': 30,
            'min_forward_speed': 10,
            'speed_at_closest_approach': 20,
            'is_slow_moving': 0,
            'is_fast_moving': 0
        }
    
    mean_speed = np.mean(speeds)
    max_speed = max(speeds)
    min_speed = min(speeds)
    
    # Speed at closest approach
    distances = [haversine(p.lat, p.lon, province_centroid.lat, province_centroid.lon) 
                 for p in track_points]
    closest_idx = np.argmin(distances)
    speed_at_closest = speeds[closest_idx] if closest_idx < len(speeds) else mean_speed
    
    return {
        'mean_forward_speed': mean_speed,
        'max_forward_speed': max_speed,
        'min_forward_speed': min_speed,
        'speed_at_closest_approach': speed_at_closest,
        'is_slow_moving': 1 if mean_speed < 15 else 0,
        'is_fast_moving': 1 if mean_speed > 40 else 0
    }
```

---

### **6.2 Track Shape & Direction Features**

| Feature Name | Description | Computation | Interpretation |
|--------------|-------------|-------------|----------------|
| `mean_direction` | Average heading | Mean of STORM_DIR | degrees (0-360) |
| `direction_variability` | Track straightness | Std of STORM_DIR | Low = straight, High = erratic |
| `is_recurving` | Major direction change | Boolean (dir change > 45¬∞) | Common in Western Pacific |
| `track_sinuosity` | Path complexity | Actual path / Straight line | 1.0 = straight, >1.0 = curved |

**Implementation:**
```python
def compute_track_shape_features(track_points):
    """
    Analyze storm track geometry
    """
    # Extract directions
    directions = [p.STORM_DIR for p in track_points 
                  if hasattr(p, 'STORM_DIR') and not np.isnan(p.STORM_DIR)]
    
    if len(directions) >= 2:
        mean_direction = circular_mean(directions)
        direction_std = circular_std(directions)
        
        # Check for recurving (major direction change)
        direction_changes = [abs(directions[i] - directions[i-1]) 
                            for i in range(1, len(directions))]
        max_change = max(direction_changes) if direction_changes else 0
        is_recurving = 1 if max_change > 45 else 0
    else:
        mean_direction = 270  # Default westward
        direction_std = 0
        is_recurving = 0
    
    # Track sinuosity
    total_distance = sum(
        haversine(track_points[i-1].lat, track_points[i-1].lon,
                 track_points[i].lat, track_points[i].lon)
        for i in range(1, len(track_points))
    )
    
    straight_line_distance = haversine(
        track_points[0].lat, track_points[0].lon,
        track_points[-1].lat, track_points[-1].lon
    )
    
    sinuosity = total_distance / straight_line_distance if straight_line_distance > 0 else 1.0
    
    return {
        'mean_direction': mean_direction,
        'direction_variability': direction_std,
        'is_recurving': is_recurving,
        'track_sinuosity': sinuosity
    }

def circular_mean(angles):
    """Compute mean of circular data (angles in degrees)"""
    angles_rad = np.radians(angles)
    return np.degrees(np.arctan2(np.mean(np.sin(angles_rad)), np.mean(np.cos(angles_rad)))) % 360

def circular_std(angles):
    """Compute std of circular data"""
    angles_rad = np.radians(angles)
    R = np.sqrt(np.mean(np.sin(angles_rad))**2 + np.mean(np.cos(angles_rad))**2)
    return np.degrees(np.sqrt(-2 * np.log(R)))
```

---

## **GROUP 6 SUMMARY: 13 Storm Motion Features**

| Subgroup | Feature Count | Key Features |
|----------|---------------|--------------|
| Forward Motion | 6 | mean_forward_speed, is_slow_moving, speed_at_closest |
| Track Shape | 4 | direction_variability, is_recurving, track_sinuosity |


---

## üîó **GROUP 7: INTERACTION FEATURES**

**Purpose**: Capture combined effects that are more predictive than individual features

**Why These Matter**: The impact isn't just distance OR intensity, it's distance AND intensity together. A weak storm very close can be worse than a strong storm far away, or vice versa.

---

### **7.1 Distance √ó Intensity Interactions**

| Feature Name | Description | Formula | Interpretation |
|--------------|-------------|---------|----------------|
| `proximity_intensity_product` | Combined threat score | `(1/min_distance¬≤) √ó max_wind_gust` | Closer + stronger = higher |
| `min_distance_x_max_wind` | Simple interaction | `min_distance √ó max_wind_gust` | Linear combination |
| `intensity_per_km` | Intensity density | `max_wind / (min_distance + 1)` | Threat concentration |

**Implementation:**
```python
def compute_distance_intensity_interactions(distance_features, weather_features):
    """
    Create interaction terms between distance and intensity
    """
    min_dist = distance_features['min_distance_km']
    max_wind = weather_features['max_wind_gust_kmh']
    
    # Avoid division by zero
    proximity = 1 / (min_dist**2 + 1)
    
    return {
        'proximity_intensity_product': proximity * max_wind,
        'min_distance_x_max_wind': min_dist * max_wind,
        'intensity_per_km': max_wind / (min_dist + 1)
    }
```

---

### **7.2 Distance √ó Weather Interactions**

| Feature Name | Description | Computation | Purpose |
|--------------|-------------|-------------|---------|
| `close_approach_rainfall` | Rain when storm is near | Precipitation when distance < 200km | Direct impact rain |
| `distant_rainfall` | Rain from outer bands | Precipitation when distance > 500km | Indirect effects |
| `rainfall_distance_ratio` | Rain concentration pattern | close_rain / total_rain | Impact distribution |

**Implementation:**
```python
def compute_distance_weather_interactions(track_points, weather_data, province_centroid):
    """
    Rainfall patterns by distance zones
    """
    # Merge distance and weather by date
    combined_data = []
    for weather_day in weather_data.itertuples():
        # Find storm positions on this day
        day_tracks = [p for p in track_points 
                     if p.timestamp.date() == weather_day.date]
        
        if len(day_tracks) > 0:
            min_dist_that_day = min(
                haversine(p.lat, p.lon, province_centroid.lat, province_centroid.lon)
                for p in day_tracks
            )
            
            combined_data.append({
                'distance': min_dist_that_day,
                'precipitation': weather_day.precipitation_sum,
                'wind': weather_day.wind_gusts_10m_max
            })
    
    # Separate into distance zones
    close_rain = sum(d['precipitation'] for d in combined_data if d['distance'] < 200)
    distant_rain = sum(d['precipitation'] for d in combined_data if d['distance'] > 500)
    total_rain = sum(d['precipitation'] for d in combined_data)
    
    rainfall_ratio = close_rain / total_rain if total_rain > 0 else 0
    
    return {
        'close_approach_rainfall': close_rain,
        'distant_rainfall': distant_rain,
        'rainfall_distance_ratio': rainfall_ratio
    }
```

---

### **7.3 Province √ó Storm Interactions**

| Feature Name | Description | Computation | Purpose |
|--------------|-------------|-------------|---------|
| `population_at_risk` | People within threat radius | Population √ó proximity_peak | Exposure quantification |
| `coastal_proximity_hazard` | Coastal province + close storm | is_coastal √ó (1/min_distance) | Storm surge risk |
| `vulnerability_exposure_score` | Historical risk √ó current threat | vulnerability_index √ó proximity | Combined risk |

**Implementation:**
```python
def compute_province_storm_interactions(province_features, distance_features, weather_features):
    """
    Combine province characteristics with storm threat
    """
    population = province_features['population']
    is_coastal = province_features['is_coastal']
    vulnerability = province_features.get('vulnerability_index', 0)
    
    proximity_peak = distance_features['proximity_peak']
    min_distance = distance_features['min_distance_km']
    max_wind = weather_features['max_wind_gust_kmh']
    
    return {
        'population_at_risk': population * proximity_peak * 1000,  # Scale for interpretability
        'coastal_proximity_hazard': is_coastal * (1 / (min_distance + 1)) * max_wind,
        'vulnerability_exposure_score': vulnerability * proximity_peak
    }
```

---


---

## **GROUP 7 SUMMARY: 11 Interaction Features**

| Subgroup | Feature Count | Key Features |
|----------|---------------|--------------|
| Distance √ó Intensity | 3 | proximity_intensity_product, intensity_per_km |
| Distance √ó Weather | 3 | close_approach_rainfall, rainfall_distance_ratio |
| Province √ó Storm | 3 | population_at_risk, coastal_proximity_hazard |


---

## üìä **COMPLETE FEATURE SUMMARY**

### **Total Feature Count: 104 Features**

| Group | Feature Count | Completeness | Importance | Implementation Status |
|-------|---------------|--------------|------------|----------------------|
| **Group 1: Distance & Proximity** | 26 | ‚úÖ 100% | üî¥ Critical | ‚úÖ Implemented |
| **Group 2: Weather Exposure** | 22 | ‚úÖ 100% | üî¥ Critical | ‚úÖ Implemented |
| **Group 3: Storm Intensity** | 10 | ‚ö†Ô∏è 40-70% | üü° Medium | ‚úÖ Implemented |
| **Group 4: Province Vulnerability** | 12 | ‚úÖ 100% | üü† High | üÜï New |
| **Group 5: Temporal & Seasonal** | 10 | ‚úÖ 100% | üü° Medium | üÜï New |
| **Group 6: Storm Motion** | 13 | ‚úÖ 95% | üü† High | üÜï New |
| **Group 7: Interactions** | 11 | ‚úÖ 100% | üü† High | üÜï New |

---

## üéØ **FEATURE PRIORITIZATION FOR MVP**

### **Tier 1: Essential Features (Start Here)**
**~35 features - Minimum viable feature set**

```python
TIER_1_FEATURES = [
    # Distance (10 features)
    'min_distance_km', 
    'mean_distance_km', 
    'hours_under_200km', 
    'hours_under_500km',
    'integrated_proximity', 
    'approach_speed_kmh', 
    'bearing_at_closest_deg',
    'did_cross_province', 
    'proximity_peak',
    'time_approaching_hours',
    
    # Weather (12 features)
    'max_wind_gust_kmh', 
    'max_wind_speed_kmh', 
    'total_precipitation_mm',
    'max_daily_precip_mm', 
    'days_with_heavy_rain', 
    'days_with_damaging_wind',
    'consecutive_rain_days', 
    'wind_rain_product', 
    'rain_during_closest_approach',
    'compound_hazard_days',
    'precipitation_concentration_index',
    'mean_daily_precipitation_mm',
    
    # Province (6 features)
    'population', 
    'population_density', 
    'is_coastal',
    'historical_avg_affected', 
    'storms_past_5yr', 
    'vulnerability_index',
    
    # Temporal (3 features)
    'month', 
    'is_peak_typhoon_season', 
    'storm_number_this_year',
    
    # Motion (2 features)
    'mean_forward_speed',
    'is_slow_moving',
    
    # Interaction (4 features)
    'proximity_intensity_product', 
    'population_at_risk', 
    'close_approach_rainfall',
    'slow_storm_rain_product'
]
# Total: 37 features
```

### **Tier 2: Enhanced Features (Add Next)**
**~35 features - Improved performance**

Add more distance thresholds, storm motion details, additional weather metrics

### **Tier 3: Advanced Features (Optional)**
**~32 features - Marginal gains**

ENSO indices, landfall detection, advanced interactions, detailed intensity evolution

---

## ‚öôÔ∏è **COMPLETE FEATURE EXTRACTION PIPELINE**

### **Master Function**
```python
def extract_all_features(storm_id, province_name, storm_date):
    """
    Complete feature extraction pipeline
    
    Parameters:
    storm_id: Unique storm identifier
    province_name: Name of province
    storm_date: Date of storm event
    
    Returns:
    dict: All 104 features
    """
    # Load raw data
    storm_track = load_storm_track(storm_id)
    weather_data = load_province_weather(province_name, storm_id)
    province_data = load_province_data(province_name, year=storm_date.year)
    impact_history = load_historical_impacts(province_name, before_date=storm_date)
    storm_history = load_storm_history(before_date=storm_date)
    
    features = {}
    
    # GROUP 1: Distance features
    features.update(compute_basic_distance_features(storm_track, province_data['centroid']))
    features.update(compute_duration_features(storm_track, province_data['centroid']))
    features.update(compute_forecast_horizon_features(storm_track, province_data['centroid']))
    features.update(compute_integrated_proximity_features(storm_track, province_data['centroid']))
    features.update(compute_approach_departure_features(storm_track, province_data['centroid']))
    features.update(compute_geometric_features(storm_track, province_data['centroid']))
    
    # GROUP 2: Weather features
    features.update(compute_peak_intensity_features(weather_data))
    features.update(compute_accumulation_duration_features(weather_data))
    closest_approach_date = get_closest_approach_date(storm_track, province_data['centroid'])
    features.update(compute_temporal_distribution_features(weather_data, closest_approach_date))
    features.update(compute_combined_hazard_features(weather_data))
    
    # GROUP 3: Storm intensity
    closest_idx = get_closest_approach_index(storm_track, province_data['centroid'])
    features.update(compute_intensity_at_critical_moments(storm_track, closest_idx))
    features.update(compute_intensity_evolution_features(storm_track, closest_idx))
    features.update(compute_intensity_data_flags(storm_track))
    
    # GROUP 4: Province vulnerability
    features.update(load_static_province_features(province_name, storm_date.year, province_data))
    features.update(compute_historical_vulnerability_features(
        province_name, storm_date, impact_history, province_data['population']
    ))
    
    # GROUP 5: Temporal features
    features.update(compute_calendar_features(storm_date))
    features.update(compute_storm_sequence_features(storm_date, storm_history))
    # Optional: features.update(get_enso_phase(storm_date, oni_data))
    
    # GROUP 6: Storm motion
    features.update(compute_forward_motion_features(storm_track, province_data['centroid']))
    features.update(compute_track_shape_features(storm_track))
    features.update(compute_landfall_features(storm_track))
    
    # GROUP 7: Interactions
    features.update(compute_distance_intensity_interactions(features, features))
    features.update(compute_distance_weather_interactions(storm_track, weather_data, province_data['centroid']))
    features.update(compute_province_storm_interactions(features, features, features))
    features.update(compute_temporal_intensity_interactions(features, features, features))
    
    # Validate and handle missing values
    features = validate_and_impute_features(features)
    
    return features

def validate_and_impute_features(features, expected_min=80):
    """
    Check feature completeness and handle missing values
    """
    # Check count
    if len(features) < expected_min:
        print(f"Warning: Only {len(features)} features generated (expected >{expected_min})")
    
    # Handle missing values
    for key, value in features.items():
        if value == -999 or pd.isna(value) or np.isinf(value):
            # Create missing value flag
            features[f'{key}_missing'] = 1
            features[key] = 0  # Or use median/mean from training set
        else:
            features[f'{key}_missing'] = 0
    
    return features
```

---

## üîç **DATA LEAKAGE PREVENTION CHECKLIST**

### **Critical Rules**

1. **Temporal Cutoff**: Historical features MUST use only data BEFORE current storm
2. **No Future Information**: Weather data must be from forecast, not reanalysis
3. **Province Updates**: Use population/characteristics from BEFORE storm year
4. **Cross-Validation**: Use temporal splits, not random splits

### **Validation Function**
```python
def check_data_leakage(features, storm_date, province_name):
    """
    Verify no future information leaked into features
    """
    assert 'historical_avg_affected' in features, "Missing historical features"
    
    # These features should NOT include current storm
    leakage_risk_features = [
        'historical_avg_affected',
        'historical_avg_destroyed',
        'historical_max_affected',
        'storms_past_5yr'
    ]
    
    # Log for manual verification
    print(f"Storm: {storm_date}, Province: {province_name}")
    for feat in leakage_risk_features:
        print(f"  {feat}: {features.get(feat, 'MISSING')}")
    
    return True
```

---

## üéì **IMPLEMENTATION BEST PRACTICES**

1. **Start Simple**: Begin with Tier 1 features (35 core features)
2. **Add Incrementally**: Add one group at a time, measure improvement
3. **Monitor Correlations**: Remove highly correlated features (>0.95)
4. **Feature Importance**: Use SHAP/permutation importance to prune
5. **Domain Knowledge**: Consult disaster management experts
6. **Operational Constraints**: Every feature must be available in real-time
7. **Missing Data Strategy**: Flag + impute, don't discard samples
8. **Normalization**: Scale features appropriately for model type
9. **Version Control**: Track feature engineering changes
10. **Documentation**: Document each feature's rationale

---

## üìà **EXPECTED FEATURE IMPORTANCE RANKING**

Based on domain knowledge and similar projects:

### **Top 10 Most Important Features (Predicted)**

1. `min_distance_km` - Distance dominates impact
2. `max_wind_gust_kmh` - Direct hazard measure
3. `total_precipitation_mm` - Flooding driver
4. `population` - Exposure base
5. `proximity_intensity_product` - Combined threat
6. `historical_avg_affected` - Vulnerability baseline
7. `is_coastal` - Geographic risk factor
8. `hours_under_200km` - Exposure duration
9. `mean_forward_speed` - Rain accumulation proxy
10. `population_at_risk` - Integrated exposure

### **Features Likely to Be Less Important**

- Fine-grained directional features (bearing variability)
- Storm intensity evolution (too much missing data)
- ENSO indices (weak signal for individual storms)
- High-order interactions (may overfit)

---

## üöÄ **NEXT STEPS**

1. **Implement Data Loaders** for all raw data sources
2. **Build Feature Pipeline** starting with Tier 1 features
3. **Validate on Historical Storm** (e.g., Yolanda 2013)
4. **Measure Feature Importance** using baseline model
5. **Iterate and Refine** based on performance

---

**End of Complete Feature Engineering Specification** üåÄ

**Total: 104 Features across 7 Groups**
**Ready for Implementation!**


